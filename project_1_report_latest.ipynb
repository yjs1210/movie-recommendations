{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "project_1_report_latest.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GplZGhBEGjNF"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yjs1210/movie-recommendations/blob/master/project_1_report_latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jVQJWYHGjMD",
        "colab_type": "text"
      },
      "source": [
        "# Movie Recommender Project Report \n",
        "<br>\n",
        "<font size=4> Team members: </font> <br><br>\n",
        "<font size=3> James Jungsuk Lee [UNI] </font><br>\n",
        "<font size=3> Ujjwal Peshin [UNI]</font><br>\n",
        "<font size=3> Bowen Zhou [UNI]</font><br>\n",
        "<font size=3> Zhongling Jiang [UNI]</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZylw9NyGwNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVLqn0a2G0XC",
        "colab_type": "code",
        "outputId": "01ad9eba-e8e3-48ee-c213-386f97c6b72a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install surprise"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: surprise in /usr/local/lib/python3.6/dist-packages (0.1)\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.6/dist-packages (from surprise) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42MIY5CjGjMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "import numpy as np\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import col, expr\n",
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T\n",
        "\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
        "\n",
        "from surprise import AlgoBase\n",
        "from surprise import Dataset\n",
        "from surprise import accuracy\n",
        "from surprise import Reader\n",
        "from surprise import KNNWithMeans\n",
        "from surprise.model_selection import KFold\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise.model_selection import cross_validate\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "import itertools\n",
        "RANDOM_SPLIT_SEED = 24\n",
        "TRAIN_PROPORTION = 0.9\n",
        "TEST_PROPORTION = 1 - TRAIN_PROPORTION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRhGLHZLGjMQ",
        "colab_type": "text"
      },
      "source": [
        "### Answer to \n",
        "As data scientists of a digital media company, state your objectives in building a\n",
        "recommendation system. For example, what metrics do you care about, who is this system\n",
        "built to serve (users or your boss?), and what business rules may you care to introduce?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6zTNa9WGjMS",
        "colab_type": "text"
      },
      "source": [
        "As a data scientist, you want to create a solution that serves the users but as well as can satisify your coworkers such as your boss. Having key stakeholders bought into your idea is integral in adding value to the company. So here we list out a few metrics and ideas that we can focus on and also point out who they serve.\n",
        "\n",
        "Users: <br/>\n",
        "We want our users to feel engaged with our content. That means we want to push out recommendations that they can relate to and enjoy. Within the context of our project, we can measure accuracy of our model such as RMSE. However, there can be some other things we can measure as well that's a bit out of scope of this project such as serendipity of our recommendations and implicit feedbacks such as how long they listen or watch our recommendations even if they don't explicitly rate them.\n",
        "    \n",
        "Stakeholders: <br/>\n",
        "Stakeholders want to make sure that the solutions that we recommend are indeed better than what they can do or have done in the past. They also want to understand how this system is affecting the product that they own. This means that measurements such as accuracy is also useful for stakeholders, but we can additionally focus on model methods that help them interpret the model's output. This means using simpler methods such as KNN or exploring Matrix Factorization methods to see if there can be any patterns in a reduced dimension space that can make sense to human rationality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xis9MIZ1GjMV",
        "colab_type": "text"
      },
      "source": [
        "## Part I: Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frpgo7-AGjMX",
        "colab_type": "text"
      },
      "source": [
        "**Description: ** <br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XML--ThGjMZ",
        "colab_type": "text"
      },
      "source": [
        "### Create a smaller development set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-Khyru-GjMb",
        "colab_type": "code",
        "outputId": "c3d314e5-2a07-4767-9b2f-23d719722134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "spark = SparkSession.builder.appName('proj_1').getOrCreate()\n",
        "ratings = spark.read.csv('/content/drive/My Drive/RecommendationSystems/ml-20m/ratings.csv', header = True, \n",
        "                         inferSchema=True).cache()\n",
        "ratings.printSchema()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- userId: integer (nullable = true)\n",
            " |-- movieId: integer (nullable = true)\n",
            " |-- rating: double (nullable = true)\n",
            " |-- timestamp: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BpyyKoPGjMf",
        "colab_type": "code",
        "outputId": "63ab408f-bdfd-46fd-f556-ac5b0f280283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "pd.DataFrame(ratings.take(5), columns=ratings.columns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112486027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484580</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userId  movieId  rating   timestamp\n",
              "0       1        2     3.5  1112486027\n",
              "1       1       29     3.5  1112484676\n",
              "2       1       32     3.5  1112484819\n",
              "3       1       47     3.5  1112484727\n",
              "4       1       50     3.5  1112484580"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ5FTjovGjMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRxggC-cGjMn",
        "colab_type": "code",
        "outputId": "f6440d6d-da6f-4436-aad8-f454f8fe8eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def subsample(movies, n, p):\n",
        "    \"\"\"\n",
        "    The function subsample the list of movies that each user rates, based on following rule:\n",
        "    (i) If user has rated fewer than n movies, we keep all ratings\n",
        "    (ii) If user has rated a lot of movies, we keep only p percent of them via random selection\n",
        "    (iii) If number of movies * p percent is less than n, we randomly sample n movies and \n",
        "         keep these ratings\n",
        "    \"\"\"\n",
        "    if len(movies) <= n:\n",
        "        return movies\n",
        "    elif int(p * len(movies)) <= n:\n",
        "        return random.sample(movies, n)\n",
        "    else:\n",
        "        return random.sample(movies, int(p * len(movies)))\n",
        "N = 5\n",
        "P = 0.2   \n",
        "# collect all movies each user has rated\n",
        "ratings_rdd = ratings.select(['userId', 'movieId']).rdd.map(list)\n",
        "users_rated_movies = ratings_rdd.groupByKey().mapValues(list)\n",
        "\n",
        "# call subsample on rated movies\n",
        "subsampled_users_rated_movies= users_rated_movies.mapValues(lambda m: subsample(m, N, P))\n",
        "\n",
        "# Now convert these ratings back to dataframe\n",
        "subsampled_ratings = subsampled_users_rated_movies.flatMapValues(lambda x: x)\n",
        "subsampled_ratings = spark.createDataFrame(subsampled_ratings, ['userId', 'movieId'])\n",
        "\n",
        "# check size of the subsampled dataset\n",
        "subsample_rowcounts = subsampled_ratings.count()\n",
        "subsample_percentage = subsample_rowcounts * 1.0 / ratings.count() * 100\n",
        "print ('The subsampled dataset is {0}% size of the original dataset, has {1} rows'.\\\n",
        "       format(subsample_percentage, subsample_rowcounts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The subsampled dataset is 19.81355445175896% size of the original dataset, has 3962763 rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqbdrnBnGjMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join them back to original dataset to get ratings and time stamps, using userId and movieId as key\n",
        "subsampled_ratings = subsampled_ratings.join(ratings, ['userId','movieid'], 'inner')\n",
        "subsampled_ratings.persist()\n",
        "pd.DataFrame(subsampled_ratings.take(5), columns=subsampled_ratings.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5LpQmAKGjMr",
        "colab_type": "text"
      },
      "source": [
        "### Divide into training, validation and test set <br>\n",
        "\n",
        "**Description**: <br><br>\n",
        "\n",
        "\n",
        "**Output**: 'train' - training set,  'val' - validation set, 'test' - test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKg65YpEGjMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subsampled_ratings = subsampled_ratings.withColumn(\"timestamp\", ratings[\"timestamp\"].cast(T.TimestampType()))\n",
        "subsampled_ratings.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GdwCobVGjMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "limit = 5\n",
        "# test size\n",
        "test_size = 0.05\n",
        "def udf_user_limit(user_counts):\n",
        "    if user_counts < limit:\n",
        "        return -1\n",
        "    else:\n",
        "        if (user_counts * (1-test_size)) <= limit:\n",
        "            return limit\n",
        "        else:\n",
        "            return int(np.around(user_counts * (1-test_size)))\n",
        "user_limit = F.udf(udf_user_limit, T.IntegerType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53l6xR1kGjMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train-test function\n",
        "def train_test_split(data, col_to_split_on= 'userId', timestamp_col = 'timestamp'):\n",
        "    # original columns\n",
        "    orig_cols = data.columns\n",
        "    # define count col\n",
        "    count_col = 'count(' + col_to_split_on + ')'\n",
        "    # do user ratings count\n",
        "    counts_for_col = data.groupby(col_to_split_on).agg(F.count(col_to_split_on))\n",
        "    # generate limits on each user based on rules\n",
        "    limits = counts_for_col.withColumn('train_limit', user_limit(F.col(count_col)))\n",
        "    # remove users having less than limit no of ratings\n",
        "    limits_filtered = limits.filter(limits.train_limit > 0)\n",
        "    # generate row numbers based on temporality\n",
        "    data_row_num = data.withColumn(\"row_num\", F.row_number().over(Window.partitionBy(col_to_split_on).orderBy(timestamp_col)))\n",
        "    # join ratings and user counts dfs together\n",
        "    data_row_num = data_row_num.alias('a')\n",
        "    limits_filtered = limits_filtered.alias('b')\n",
        "    merged_data = data_row_num.join(limits_filtered,F.col('b.' + col_to_split_on) == F.col('a.' + col_to_split_on)).select([F.col('a.'+xx) for xx in data_row_num.columns] + [F.col('b.' + count_col),F.col('b.train_limit')])\n",
        "    # generate selection column based on number limit\n",
        "    final_train_test = merged_data.withColumn('selection', F.col('row_num') <= F.col('train_limit'))\n",
        "    # find train and test \n",
        "    train = final_train_test.filter(final_train_test.selection == True).select(orig_cols)\n",
        "    test = final_train_test.filter(final_train_test.selection == False).select(orig_cols)\n",
        "    return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D55i4IKyGjMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to check if test has more entities in any column\n",
        "def compatibility_test(train, test, cols_to_test = ['userId', 'movieId']):\n",
        "    cols_greater = []\n",
        "    for i in cols_to_test:\n",
        "        train_unique = train.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n",
        "        test_unique = test.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n",
        "        size = len(list(set(test_unique) - set(train_unique)))\n",
        "        print(\"Test has %d more %s\" %(size, i))\n",
        "        cols_greater.append(i)\n",
        "    return cols_greater"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOXYndh6GjMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traincv, test = train_test_split(subsampled_ratings, col_to_split_on= 'movieId', timestamp_col='timestamp')\n",
        "print((traincv.count(), len(traincv.columns)))\n",
        "print((test.count(), len(test.columns)))\n",
        "cols_greater = compatibility_test(traincv, test, cols_to_test=['userId', 'movieId'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnQ8mIHvGjMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in cols_greater:\n",
        "    traincv_unique = traincv.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n",
        "    test_unique = test.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n",
        "    unique_to_test = list(set(test_unique) - set(traincv_unique))\n",
        "    test = test[~test[i].isin(unique_to_test)]\n",
        "# After removing users/ movies not included in training set\n",
        "print((traincv.count(), len(traincv.columns)))\n",
        "print((test.count(), len(test.columns)))\n",
        "cols_greater = compatibility_test(traincv, test, cols_to_test=['userId', 'movieId'])\n",
        "test.persist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkvZR-DuGjM0",
        "colab_type": "text"
      },
      "source": [
        "Repeat the same logic on train / validation set split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT0sSyLPGjM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, val = train_test_split(traincv, col_to_split_on= 'movieId', timestamp_col='timestamp')\n",
        "print((train.count(), len(train.columns)))\n",
        "print((val.count(), len(val.columns)))\n",
        "cols_greater = compatibility_test(train, val, cols_to_test=['userId', 'movieId'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZJjXJfxGjM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in cols_greater:\n",
        "    train_unique = train.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n",
        "    val_unique = val.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n",
        "    unique_to_val = list(set(val_unique) - set(train_unique))\n",
        "    val = val[~val[i].isin(unique_to_val)]\n",
        "print((train.count(), len(train.columns)))\n",
        "print((val.count(), len(val.columns)))\n",
        "cols_greater = compatibility_test(train, val, cols_to_test=['userId', 'movieId'])\n",
        "train.persist()\n",
        "val.persist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKtl9wwjGjM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train.write.csv('gs://moviercommendation/train.csv')\n",
        "# val.write.csv('gs://moviercommendation/val.csv')\n",
        "# test.write.csv('gs://moviercommendation/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmyg22CcGjM5",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation Metrics\n",
        "\n",
        "We explored mainly two type of evaluation metrics on our recommender system -- regression metrics and ranking metrics.\n",
        "The problem setting can be described as the following: \n",
        "\n",
        "Assume U is a set of M users, $U=\\{u_0,u_1,...,u_{M−1}\\}$. \n",
        "\n",
        "Each user $u_i$ having a set of N ground truth relevant movies $D_i=\\{d_0,d_1,...,d_{N−1}\\}$ and a list of Q recommended movies, in order of decreasing relevance $Ri=\\{r_0,r_1,...,r_{Q−1}\\}$\n",
        "\n",
        "Our goal is to evaluate how relevant our recommended movies are for each user. The relevance of the sets and the effectiveness of the algorithms can be measured using the metrics listed below.\n",
        "\n",
        "Before diving into definition of metrics, we define a indicator function which, provided a recommended document and a set of ground truth relevant documents, returns a 0/1 score to indicate relevance.\n",
        "\n",
        "$rel_D(r)$ = \\begin{cases}\n",
        "    1,& \\text{if } r\\in D\\\\\n",
        "    0,              & \\text{otherwise}\n",
        "\\end{cases}\n",
        "\n",
        "\n",
        "#### I. Regression Metric\n",
        "\n",
        "* Root Mean Square Error (RMSE): $\\frac{\\sqrt{\\sum_{i=0}^{N - 1}e_{i}^2}}{N}$\n",
        "* Mean Absolute Error (MAE): $\\frac{\\sum_{i=0}^{N - 1}|e_{i}|}{N}$\n",
        "* R-squared: $1 - \\frac{\\sum_{i=0}^{N - 1} (e_i)^2 }{\\sum_{i=0}^{N - 1} (y_i - \\bar{y}_i)^2}$\n",
        "* Explained Variance: $1 - \\frac{Var(y - \\hat{y})}{Var(y)} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDSnVmRyGjM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regression_metric(ratings, predicted_ratings):\n",
        "    \"\"\"\n",
        "        Calculate regression metrics\n",
        "        - root mean square error\n",
        "        - mean absoluate error\n",
        "        - r squared\n",
        "        - explained variance\n",
        "        by joining original and predicted rating dataset into \n",
        "        ((userId, movieId), (original_rating, predicted_rating)).\n",
        "        :ratings: original rating dataset. Format:   userId | movieId | rating\n",
        "        :predicted_ratings: predicted rating dataset\n",
        "    \"\"\"\n",
        "    ratings_tuple = ratings.rdd.map(lambda r: ((r.userId, r.movieId), r.rating))\n",
        "    predicted_ratings_tuple = predicted_ratings.rdd.map(lambda r: ((r.userId, r.movieId), r.rating))\n",
        "    score_and_labels = predicted_ratings_tuple.join(ratings_tuple).map(lambda tup: tup[1])\n",
        "    metrics = RegressionMetrics(score_and_labels)\n",
        "    print(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n",
        "    print(\"MAE = %s\" % metrics.meanAbsoluteError)\n",
        "    print(\"R-squared = %s\" % metrics.r2)\n",
        "    print(\"Explained Variance = %s\" % metrics.explainedVariance)\n",
        "    return metrics.rootMeanSquaredError, metrics.meanAbsoluteError, metrics.r2, metrics.explainedVariance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umg7IBZXGjM7",
        "colab_type": "text"
      },
      "source": [
        "#### II. Ranking Metric\n",
        "\n",
        "* Precision @ k: measures on average what proportion of first k recommended movies to users are contained in true set of relevant movies to each user, i.e.<br>\n",
        "\n",
        "<font size=2.5>$p(k) = \\frac{1}{M}\\sum_{i=0}^{M-1}\\frac{1}{k}\\sum_{j=0}^{min(|D|, k)-1}rel_{D_i}(R_i(j))$</font>\n",
        "\n",
        "* Mean Average Precision @ k: measures similar quantity as Precision @ k except imposing penalty regarding order of recommendation, i.e. <br>\n",
        "\n",
        "<font size=2.5>$p(k) = \\frac{1}{M}\\sum_{i=0}^{M-1}\\frac{1}{k}\\sum_{j=0}^{min(|D|, k)-1}\\frac{rel_{D_i}(R_i(j))}{j + 1}$</font>\n",
        "\n",
        "* Normalized Discounted Continuous Gain (NDCG): measures how many of the first k recommended moveis are in the set of true relevant documents averaged across all users, while taking into account the order of the recommendations (movies are assumed to be in order of decreasing relevance).\n",
        "\n",
        "<font size=2.5>$NDCG(k) = \\frac{1}{M}\\sum_{i=0}^{M-1} \\frac{1}{IDCG(D_i, k)}\\sum_{j=0}^{n-1}\\frac{rel_{D_i}(R_i(j))}{ln(j + 1)} $<font> <br>\n",
        "where $IDCG(k) = \\sum_{j=0}^{min(|D|, k)-1}\\frac{rel_{D_i}(R_i(j))}{ln(j + 1)}$ represents best possible attained by the most ideal ranking.\n",
        "    \n",
        "**Procedure**:\n",
        "\n",
        "(i) Filter out movies with rating score < r, because we are only interested in evaluating relevant movies, or 'good' recommendations. \n",
        "\n",
        "(ii) Sort ground truths and predictions by ratings (predicted ratings) and retrieve top k movies as reccommended items for a user. If in test set, a user rates fewer than k movies, then only retrieve all his rated movies.\n",
        "\n",
        "\n",
        "(iii) Calculate specific ranking metric on test set and predicted set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRQyDgQbGjM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ranking metrics\n",
        "# https://vinta.ws/code/spark-ml-cookbook-pyspark.html\n",
        "def ranking_metric(ratings, predicted_ratings, r=3, k=5):\n",
        "    \"\"\"\n",
        "        Calculate ranking metrics\n",
        "        - mean average precision\n",
        "        - precision at k\n",
        "        - normalized discounted continuous gain at k\n",
        "        by collecting recommended items and then comparing them to relevant groud truth items, \n",
        "        which takes form of\n",
        "        ((userId, movieId), ([[predicted_item1, predicted_item2, ...], [item1, item2, ...]))\n",
        "        :ratings: original rating dataset. Format:   userId | movieId | rating\n",
        "        :predicted ratings: predicted rating dataset, same format\n",
        "        :r: threshold for rating that defines a 'relevant' movie\n",
        "        :k: top k positions used for comparison\n",
        "    \"\"\"\n",
        "    windowSpec = Window.partitionBy('userId').orderBy(col('rating').desc())\n",
        "    perUserPredictedItemsDF = predicted_ratings \\\n",
        "        .where('rating >= {0}'.format(r)) \\\n",
        "        .select('userId', 'movieId', 'rating', F.rank().over(windowSpec).alias('rank')) \\\n",
        "        .where('rank <= {0}'.format(k)) \\\n",
        "        .groupBy('userId') \\\n",
        "        .agg(expr('collect_list(movieId) as itemsId'))\n",
        "    \n",
        "    windowSpec = Window.partitionBy('userId').orderBy(col('rating').desc())\n",
        "    perUserActualItemsDF = ratings \\\n",
        "        .where('rating >= {0}'.format(r)) \\\n",
        "        .select('userId', 'movieId', 'rating', F.rank().over(windowSpec).alias('rank')) \\\n",
        "        .where('rank <= {0}'.format(k)) \\\n",
        "        .groupBy('userId') \\\n",
        "        .agg(expr('collect_list(movieId) as itemsId')) \n",
        "    \n",
        "    perUserItemsRDD = perUserPredictedItemsDF.join(perUserActualItemsDF, 'userId') \\\n",
        "        .rdd \\\n",
        "        .map(lambda row: (row[1], row[2]))\n",
        "    rankingMetrics = RankingMetrics(perUserItemsRDD)\n",
        "\n",
        "    print(\"mean Average Precision = %s\" %rankingMetrics.meanAveragePrecision)\n",
        "    print(\"Precision at k = %s\" %rankingMetrics.precisionAt(k))\n",
        "    print(\"NDCG at k = %s\" %rankingMetrics.ndcgAt(k))  \n",
        "    \n",
        "    return rankingMetrics.meanAveragePrecision, rankingMetrics.precisionAt(k), rankingMetrics.ndcgAt(k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxHGQMa8GjM8",
        "colab_type": "text"
      },
      "source": [
        "#### III. Coverage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_oNp0dEGjM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def user_coverage_metric(ratings, predicted_ratings, r=3, k=5):\n",
        "    windowSpec = Window.partitionBy('userId').orderBy(col('rating').desc())\n",
        "    perUserPredictedItemsDF = predicted_ratings \\\n",
        "        .where('rating >= {0}'.format(r)) \\\n",
        "        .select(col('userId').alias('p_userId'), col('movieId').alias('p_movieId'), \\\n",
        "                F.col('rating').alias('p_ratings'), F.rank().over(windowSpec).alias('rank'))  \n",
        "    # join with test set ratings and rename columns\n",
        "    joined = perUserPredictedItemsDF \\\n",
        "        .join(val, (perUserPredictedItemsDF.p_userId ==val.userId) & \\\n",
        "              (perUserPredictedItemsDF.p_movieId == val.movieId))     \n",
        "    # the number of users have above threshold, 'successful' recommendations\n",
        "    covered_users = joined.where('rating >= {0}'.format(r)) \\\n",
        "        .groupBy('p_userId') \\\n",
        "        .agg(F.countDistinct('p_movieId').alias('count')) \\\n",
        "        .where('count >= {0}'.format(k)) \\\n",
        "        .agg(F.countDistinct('p_userId'))    \n",
        "    covered_users = covered_users.collect()[0][0]\n",
        "    # the numeber of user have above threshold groudtruths \n",
        "    actual = val.where('rating >= {0}'.format(r)) \\\n",
        "        .groupBy('userId') \\\n",
        "        .agg(F.countDistinct('movieId').alias('count')) \\\n",
        "        .where('count >= {0}'.format(k)) \\\n",
        "        .agg(F.countDistinct('userId'))\n",
        "    total_users = actual.collect()[0][0]\n",
        "    if total_users == 0:\n",
        "        user_coverage = -1\n",
        "        print('No user with equal to or greater than {0} items'.format(k))\n",
        "    else:\n",
        "        user_coverage = covered_users * 1.0 / total_users\n",
        "    return user_coverage\n",
        "            \n",
        "def item_coverage_metric(ratings, predicted_ratings, r=3, k=5):\n",
        "    windowSpec = Window.partitionBy('movieId').orderBy(col('rating').desc())\n",
        "    perUserPredictedItemsDF = predicted_ratings \\\n",
        "        .where('rating >= {0}'.format(r)) \\\n",
        "        .select(col('userId').alias('p_userId'), col('movieId').alias('p_movieId'), \\\n",
        "                F.col('rating').alias('p_ratings'), F.rank().over(windowSpec).alias('rank'))  \n",
        "    # join with test set ratings and rename columns\n",
        "    joined = perUserPredictedItemsDF \\\n",
        "        .join(val, (perUserPredictedItemsDF.p_userId ==val.userId) & \\\n",
        "              (perUserPredictedItemsDF.p_movieId == val.movieId))     \n",
        "    # the number of users have above threshold, 'successful' recommendations\n",
        "    covered_items = joined.where('rating >= {0}'.format(r)) \\\n",
        "        .groupBy('p_movieId') \\\n",
        "        .agg(F.countDistinct('p_userId').alias('count')) \\\n",
        "        .where('count >= {0}'.format(k)) \\\n",
        "        .agg(F.countDistinct('p_movieId'))    \n",
        "    covered_users = covered_items.collect()[0][0]\n",
        "    # the numeber of user have above threshold groudtruths \n",
        "    actual = val.where('rating >= {0}'.format(r)) \\\n",
        "        .groupBy('movieId') \\\n",
        "        .agg(F.countDistinct('userId').alias('count')) \\\n",
        "        .where('count >= {0}'.format(k)) \\\n",
        "        .agg(F.countDistinct('movieId'))\n",
        "    total_items = actual.collect()[0][0]\n",
        "    if total_items == 0:\n",
        "        item_coverage = -1 \n",
        "        print('No item rated by equal to or greater than {0} users'.format(k))\n",
        "    else:\n",
        "        item_coverage = covered_items * 1.0 / total_items\n",
        "    return item_coverage\n",
        "\n",
        "def catelog_coverage_metric(ratings, predicted_ratings, r=3, k=5):\n",
        "    windowSpec = Window.partitionBy('userId').orderBy(col('rating').desc())\n",
        "    perUserPredictedItemsDF = predicted_ratings \\\n",
        "        .where('rating >= {0}'.format(r)) \\\n",
        "        .select('userId', 'movieId', 'rating', F.rank().over(windowSpec).alias('rank')) \\\n",
        "        .where('rank <= {0}'.format(k)) \\\n",
        "        .agg(F.countDistinct('movieId'))\n",
        "    num_rec_movies = perUserPredictedItemsDF.collect()[0][0]\n",
        "    \n",
        "    perUserActualItemsDF = ratings \\\n",
        "        .select('movieId') \\\n",
        "        .agg(F.countDistinct('movieId'))\n",
        "    num_total_movies = perUserActualItemsDF.collect()[0][0]\n",
        "    return num_rec_movies * 1.0 / num_total_movies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU-SmDbXGjM-",
        "colab_type": "text"
      },
      "source": [
        "### Baseline Model <br>\n",
        "\n",
        "**Description:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PEMtWQTGjM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineModel:\n",
        "\n",
        "    def __init__(self, user_column, item_column,ratings_column):\n",
        "        self.user_col = user_column\n",
        "        self.item_col = item_column\n",
        "        self.ratings_col = ratings_column\n",
        "\n",
        "    def __find_avg_of_col(self, data, column):\n",
        "        return data.select(F.mean(F.col(column))).collect()[0][0]\n",
        "\n",
        "    def __subtract_from_col(self, data, column, value):\n",
        "        return data.withColumn('normalized_' + column, F.col(column)-value)\n",
        "\n",
        "    def train(self, training_data):\n",
        "        user_col = self.user_col\n",
        "        item_col = self.item_col\n",
        "        ratings_col = self.ratings_col\n",
        "        user_bias = {}\n",
        "        item_bias = {}\n",
        "\n",
        "        # find average and calculate bias for user and item\n",
        "        avg_rating = self.__find_avg_of_col(training_data, ratings_col)\n",
        "        norm_training_data = self.__subtract_from_col(training_data, ratings_col, avg_rating)\n",
        "        user_bias = norm_training_data.groupby(user_col).agg(F.avg('normalized_' + ratings_col)).\\\n",
        "                                                    rdd.map(lambda x : (x[0],x[1])).collectAsMap()\n",
        "        item_bias = norm_training_data.groupby(item_col).agg(F.avg('normalized_' + ratings_col)).\\\n",
        "                                                    rdd.map(lambda x : (x[0],x[1])).collectAsMap()\n",
        "\n",
        "        self.training_data = norm_training_data\n",
        "        self.avg_rating = avg_rating\n",
        "        self.user_bias = user_bias\n",
        "        self.item_bias = item_bias\n",
        "    \n",
        "        return avg_rating, user_bias, item_bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6CRAuw8GjNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baselineModel = BaselineModel(user_column='userId', item_column='movieId', ratings_column='rating')\n",
        "avg_rating, user_bias, item_bias = baselineModel.train(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_KwwwaEGjNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def udf_predict(user_id, item_id):\n",
        "    \"\"\"\n",
        "        Predict function: average rating + user bias + item bias. Handle\n",
        "        corner cases where there is missing user id and item id. \n",
        "    \"\"\"\n",
        "    if (user_id in user_bias) and (item_id in item_bias) :\n",
        "        tmp = avg_rating + user_bias[user_id] + item_bias[item_id]\n",
        "    elif user_id in user_bias:\n",
        "        tmp = avg_rating + user_bias[user_id]\n",
        "    elif item_id in item_bias:\n",
        "        tmp = avg_rating + item_bias[item_id]\n",
        "    else: \n",
        "        tmp = avg_rating\n",
        "    if tmp > 5.0:\n",
        "        return 5.0\n",
        "    elif tmp < 0.5:\n",
        "        return 0.5\n",
        "    else:\n",
        "        return tmp\n",
        "def predict(test_data, user_column, item_column, ratings_column):\n",
        "    predict_udf = F.udf(udf_predict, T.FloatType())\n",
        "    return test_data.withColumn(ratings_column, predict_udf(F.col(user_column), F.col(item_column)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snVqvxp2GjNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Do cross join so we can predict on everything, not just the data that is available in the test set\n",
        "# user_vals = test.select('userId')\n",
        "# rating_vals = test.select('movieId')\n",
        "# predict_df = user_vals.crossJoin(rating_vals)\n",
        "predicted_ratings_baseline = predict(val, user_column='userId', item_column='movieId', ratings_column='rating')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJBaMJK1GjND",
        "colab_type": "code",
        "outputId": "6dbc08f9-015e-4e6b-aacf-d3ffdeea323c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "rmse_baseline, mae_baseline, r2_baseline, explainedvar_baseline = \\\n",
        "regression_metric(val, predicted_ratings_baseline)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE = 0.9250013594202334\n",
            "MAE = 0.6849676576562459\n",
            "R-squared = 0.20382442522263133\n",
            "Explained Variance = 0.6128602941096273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1Z1Kpr3CGjNE",
        "colab_type": "code",
        "outputId": "ae9926e9-134e-4dc7-c119-0a172f444d9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "averageprecision_baseline, precision_baseline, ndcg_baseline = \\\n",
        "ranking_metric(val, predicted_ratings_baseline, k = 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean Average Precision = 0.8982800498081532\n",
            "Precision at k = 0.046447118891320235\n",
            "NDCG at k = 0.9275626584151085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJaZ5iPzJINv",
        "colab_type": "code",
        "outputId": "07a741ae-e1a4-41f9-bc0b-0aefa246cd0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "user_cover = user_coverage_metric(val, predicted_ratings_baseline, k = 10)\n",
        "print('user coverage = ', user_cover)\n",
        "\n",
        "# item_cover = item_coverage_metric(val, predicted_ratings_baseline, k = 5)\n",
        "# print('item coverage = ', item_cover)\n",
        "\n",
        "cate_cover = catelog_coverage_metric(val, predicted_ratings_baseline, k = 10)\n",
        "print('catalog coverage = ', cate_cover)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-d458a9f5a08a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muser_cover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_coverage_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_ratings_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user coverage = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_cover\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# item_cover = item_coverage_metric(val, predicted_ratings_baseline, k = 5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print('item coverage = ', item_cover)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predicted_ratings_baseline' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GplZGhBEGjNF",
        "colab_type": "text"
      },
      "source": [
        "### Item Based Model <br> <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjDMajSYGjNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = train.toPandas()\n",
        "val_df = val.toPandas()\n",
        "reader = Reader(rating_scale=(0.5,5.0))\n",
        "train_data = Dataset.load_from_df(train_df[['userId','movieId','rating']],reader)\n",
        "val_data = Dataset.load_from_df(val_df[['userId','movieId','rating']], reader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HCDBpKhGjNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = train_data.build_full_trainset()\n",
        "#val_data = val_data.build_anti_testset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkC5erR7GjNI",
        "colab_type": "code",
        "outputId": "c81e5365-2be8-4207-fcdb-0153ce64e581",
        "colab": {}
      },
      "source": [
        "sim_options = {'name':'cosine', 'user_based':False}\n",
        "algo = KNNWithMeans(sim_options = sim_options)\n",
        "algo.fit(train_data)\n",
        "# predictions = algo.test(val_df)\n",
        "# accuracy.rmse(predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.knns.KNNWithMeans at 0x7f032fd7fb10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lde_m104GjNJ",
        "colab_type": "code",
        "outputId": "34cc87a2-d3d1-447c-a3e2-c1e6c935cd24",
        "colab": {}
      },
      "source": [
        "val_data = val_data.build_full_trainset().build_testset()\n",
        "predictions = algo.test(val_df)\n",
        "accuracy.rmse(predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-54febee9343a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_full_trainset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_testset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/anaconda/lib/python2.7/site-packages/surprise/prediction_algorithms/algo_base.pyc\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, testset, verbose)\u001b[0m\n\u001b[1;32m    165\u001b[0m                                     \u001b[0mr_ui_trans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                                     verbose=verbose)\n\u001b[0;32m--> 167\u001b[0;31m                        for (uid, iid, r_ui_trans) in testset]\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpusZoCiGjNL",
        "colab_type": "text"
      },
      "source": [
        "### Matrix Factorization Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU6QiBUBGjNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example Run: ALS with random chosen parameters\n",
        "als = ALS(maxIter=10, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
        "          coldStartStrategy=\"drop\")\n",
        "als_model = als.fit(train)\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
        "                                predictionCol=\"prediction\")\n",
        "predicted_ratings_als = als_model.transform(val)\n",
        "predicted_ratings_als = \\\n",
        "    predicted_ratings_als.withColumn('rating', predicted_ratings_als.prediction).drop('prediction')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2mcxmL6GjNM",
        "colab_type": "code",
        "outputId": "c5ac9e26-dcf6-4b1f-fb6f-12d5f190d822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "rmse_als, mae_als, r2_als, explainedvar_als= \\\n",
        "regression_metric(val, predicted_ratings_als)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE = 1.1589308675610952\n",
            "MAE = 0.8592929290569877\n",
            "R-squared = -0.24738609096262865\n",
            "Explained Variance = 1.0430436441844482\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60TRBy18GjNO",
        "colab_type": "code",
        "outputId": "27ba40b2-154a-4f85-f56b-a9b66b748cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Re-run the cell when there is an error\n",
        "averageprecision_als, precision_als, ndcg_als = \\\n",
        "ranking_metric(val, predicted_ratings_als, k = 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean Average Precision = 0.7236012764392398\n",
            "Precision at k = 0.3123523046959532\n",
            "NDCG at k = 0.8041489058790465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkKaUGUDUgqJ",
        "colab_type": "code",
        "outputId": "07095127-8d06-473f-8124-c788f2c1fad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "user_cover = user_coverage_metric(val, predicted_ratings_als, k = 10)\n",
        "print('user coverage = ', user_cover)\n",
        "\n",
        "# item_cover = item_coverage_metric(val, predicted_ratings_baseline, k = 5)\n",
        "# print('item coverage = ', item_cover)\n",
        "\n",
        "cate_cover = catelog_coverage_metric(val, predicted_ratings_als, k = 10)\n",
        "print('catalog coverage = ', cate_cover)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "user coverage =  0.6656126482213439\n",
            "catalog coverage =  0.7183965072435007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3VXeHO2GjNQ",
        "colab_type": "text"
      },
      "source": [
        "#### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33a_DIpZGjNQ",
        "colab_type": "code",
        "outputId": "b29b1764-8d23-4a92-c0cd-7c47c0b8e484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        "CAUTION: THIS CELL TAKES LONG TO RUN\n",
        "Hyperparameter tuning using one validation set. \n",
        "\"\"\"\n",
        "from time import time\n",
        "grid = {'maxIter':[50], 'rank':[5, 20, 50], 'regParam': [0.1, 0.5, 0.01]}\n",
        "param_vals = []\n",
        "for key,v in grid.items():\n",
        "    param_vals.append(v)\n",
        "    \n",
        "final_results = dict()\n",
        "for i in itertools.product(*param_vals):\n",
        "    print(i)\n",
        "    evaluation = []\n",
        "    start = time()\n",
        "    \n",
        "    inputs = dict()\n",
        "    for j,(key,v) in enumerate(grid.items()):\n",
        "        inputs[key] = i[j]\n",
        "    inputs['userCol'] ='userId'\n",
        "    inputs['itemCol'] = 'movieId'\n",
        "    inputs['ratingCol'] = 'rating'\n",
        "    inputs['coldStartStrategy'] = 'drop'\n",
        "    als = ALS(**inputs)\n",
        "    model = als.fit(train)\n",
        "    predictions = model.transform(val)\n",
        "    predictions = predictions.withColumn('rating', predictions.prediction).drop('prediction')\n",
        "    cv_rmse, cv_mae, cv_rw, cv_exp_var = \\\n",
        "    regression_metric(val, predictions)\n",
        "    cv_averageprecision, cv_precision, cv_ndcg = \\\n",
        "    ranking_metric(val, predictions, k = 10)\n",
        "    user_cover = user_coverage_metric(val, predictions, k = 10)\n",
        "    cate_cover = catelog_coverage_metric(val, predictions, k = 10)\n",
        "    print('user coverage = ', user_cover)\n",
        "    print('catalog coverage = ', cate_cover)\n",
        "\n",
        "    evaluation.append({'rmse': cv_rmse, 'mae': cv_mae, 'rw': cv_rw, 'exp_var': cv_exp_var,\n",
        "                       'avg_pres' : cv_averageprecision, 'pres': cv_precision, 'ndcg': cv_ndcg, \n",
        "                       'user_cov': user_cover, 'cate_cover' : cate_cover})\n",
        "    print(time() - start)\n",
        "    final_results[i] = evaluation\n",
        "\n",
        "# If you want to run hyperparameter tuning using cross validation. This takes even longer time.\n",
        "# paramMap = ParamGridBuilder() \\\n",
        "#                     .addGrid(als.rank, [10, 50, 100]) \\\n",
        "#                     .addGrid(als.maxIter, [10]) \\\n",
        "#                     .addGrid(als.regParam, [0.01,0.001]) \\\n",
        "#                     .build()\n",
        "\n",
        "# evaluatorR = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\")\n",
        "\n",
        "# # Run cross-validation, and choose the best set of parameters.\n",
        "# cv = CrossValidator(estimator=als,\n",
        "#                             estimatorParamMaps=paramMap,\n",
        "#                             evaluator=evaluatorR,\n",
        "#                            numFolds=5)\n",
        "\n",
        "# cv_res = cv.fit(subsampled_train)\n",
        "# final_results\n",
        "# predicted_ratings_als = cv_res.bestModel.transform(subsampled_test)\n",
        "# predicted_ratings_als = predicted_ratings_als.withColumn('rating', predicted_ratings_als.prediction).drop('prediction')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 5, 0.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-5b66498e83ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coldStartStrategy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'drop'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4132.fit.\n: org.apache.spark.SparkException: Job 62 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:932)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:930)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:930)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2128)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2041)\n\tat org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1948)\n\tat org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:1031)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$fit$1.apply(ALS.scala:676)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$fit$1.apply(ALS.scala:658)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:658)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:569)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yR3ncYOGjNR",
        "colab_type": "code",
        "outputId": "d5d7fc58-1b7e-4223-f34a-ae09d7766f1e",
        "colab": {}
      },
      "source": [
        "final_results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0.1, 20, 5): [{'rmse': 0.8263763350747986}],\n",
              " (0.1, 20, 20): [{'rmse': 0.8148016939541032}],\n",
              " (0.1, 100, 5): [{'rmse': 0.8474903644526278}],\n",
              " (0.1, 100, 20): [{'rmse': 0.8154197942484954}],\n",
              " (2.0, 20, 5): [{'rmse': 2.7463870845485077}],\n",
              " (2.0, 20, 20): [{'rmse': 2.084787577417859}],\n",
              " (2.0, 100, 5): [{'rmse': 2.7503422735344567}],\n",
              " (2.0, 100, 20): [{'rmse': 2.0847875768277393}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUXkynU_GjNS",
        "colab_type": "code",
        "outputId": "bc333a3a-2ff4-45ec-ae05-56341016fd5c",
        "colab": {}
      },
      "source": [
        "grid = {'maxIter':[5, 20], 'rank':[20, 100], 'regParam': [0.1,2.0]}\n",
        "param_vals = []\n",
        "for key,v in grid.items():\n",
        "    param_vals.append(v)\n",
        "    \n",
        "final_results = dict()\n",
        "for i in itertools.product(*param_vals):\n",
        "    print i"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.1, 20, 5)\n",
            "(0.1, 20, 20)\n",
            "(0.1, 100, 5)\n",
            "(0.1, 100, 20)\n",
            "(2.0, 20, 5)\n",
            "(2.0, 20, 20)\n",
            "(2.0, 100, 5)\n",
            "(2.0, 100, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG2Wpy_4GjNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# averageprecision_als, precision_als, ndcg_als = \\\n",
        "# ranking_metric(val, predicted_ratings_als)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzvGlHl5GjNU",
        "colab_type": "text"
      },
      "source": [
        "## Performance Comparison on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucC3BKtGjNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combine train and cv\n",
        "\n",
        "\n",
        "# run three models on traincv and evaluate on test\n",
        "\n",
        "\n",
        "# Create visualization of metrics change"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5VDiTkwGjNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}