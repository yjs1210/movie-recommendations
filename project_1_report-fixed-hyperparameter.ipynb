{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommender Project Report \n",
    "<br>\n",
    "<font size=4> Team members: </font> <br><br>\n",
    "<font size=3> James Jungsuk Lee [UNI] </font><br>\n",
    "<font size=3> Ujjwal Peshin [UNI]</font><br>\n",
    "<font size=3> Bowen Zhou [UNI]</font><br>\n",
    "<font size=3> Zhongling Jiang [UNI]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, expr\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "'''\n",
    "from surprise import AlgoBase\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise import Reader\n",
    "from surprise import KNNBasic\n",
    "from surprise.model_selection import KFold\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import cross_validate\n",
    "import pandas as pd\n",
    "import random\n",
    "'''\n",
    "\n",
    "import itertools\n",
    "RANDOM_SPLIT_SEED = 24\n",
    "TRAIN_PROPORTION = 0.9\n",
    "TEST_PROPORTION = 1 - TRAIN_PROPORTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to \n",
    "As data scientists of a digital media company, state your objectives in building a\n",
    "recommendation system. For example, what metrics do you care about, who is this system\n",
    "built to serve (users or your boss?), and what business rules may you care to introduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a data scientist, you want to create a solution that serves the users but as well as can satisify your coworkers such as your boss. Having key stakeholders bought into your idea is integral in adding value to the company. So here we list out a few metrics and ideas that we can focus on and also point out who they serve.\n",
    "\n",
    "Users: <br/>\n",
    "We want our users to feel engaged with our content. That means we want to push out recommendations that they can relate to and enjoy. Within the context of our project, we can measure accuracy of our model such as RMSE. However, there can be some other things we can measure as well that's a bit out of scope of this project such as serendipity of our recommendations and implicit feedbacks such as how long they listen or watch our recommendations even if they don't explicitly rate them.\n",
    "    \n",
    "Stakeholders: <br/>\n",
    "Stakeholders want to make sure that the solutions that we recommend are indeed better than what they can do or have done in the past. They also want to understand how this system is affecting the product that they own. This means that measurements such as accuracy is also useful for stakeholders, but we can additionally focus on model methods that help them interpret the model's output. This means using simpler methods such as KNN or exploring Matrix Factorization methods to see if there can be any patterns in a reduced dimension space that can make sense to human rationality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description: ** <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a smaller development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('proj_1').getOrCreate()\n",
    "ratings = spark.read.csv('ml-20m/ratings.csv', header = True, \n",
    "                         inferSchema=True).cache()\n",
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112486027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1        2     3.5  1112486027\n",
       "1       1       29     3.5  1112484676\n",
       "2       1       32     3.5  1112484819\n",
       "3       1       47     3.5  1112484727\n",
       "4       1       50     3.5  1112484580"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ratings.take(5), columns=ratings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subsampled dataset is 19.81355445175896% size of the original dataset, has 3962763 rows\n"
     ]
    }
   ],
   "source": [
    "def subsample(movies, n, p):\n",
    "    \"\"\"\n",
    "    The function subsample the list of movies that each user rates, based on following rule:\n",
    "    (i) If user has rated fewer than n movies, we keep all ratings\n",
    "    (ii) If user has rated a lot of movies, we keep only p percent of them via random selection\n",
    "    (iii) If number of movies * p percent is less than n, we randomly sample n movies and \n",
    "         keep these ratings\n",
    "    \"\"\"\n",
    "    if len(movies) <= n:\n",
    "        return movies\n",
    "    elif int(p * len(movies)) <= n:\n",
    "        return random.sample(movies, n)\n",
    "    else:\n",
    "        return random.sample(movies, int(p * len(movies)))\n",
    "N = 5\n",
    "P = 0.2    \n",
    "# collect all movies each user has rated\n",
    "ratings_rdd = ratings.select(['userId', 'movieId']).rdd.map(list)\n",
    "users_rated_movies = ratings_rdd.groupByKey().mapValues(list)\n",
    "\n",
    "# call subsample on rated movies\n",
    "subsampled_users_rated_movies= users_rated_movies.mapValues(lambda m: subsample(m, N, P))\n",
    "\n",
    "# Now convert these ratings back to dataframe\n",
    "subsampled_ratings = subsampled_users_rated_movies.flatMapValues(lambda x: x)\n",
    "subsampled_ratings = spark.createDataFrame(subsampled_ratings, ['userId', 'movieId'])\n",
    "\n",
    "# check size of the subsampled dataset\n",
    "subsample_rowcounts = subsampled_ratings.count()\n",
    "subsample_percentage = subsample_rowcounts * 1.0 / ratings.count() * 100\n",
    "print ('The subsampled dataset is {0}% size of the original dataset, has {1} rows'.\\\n",
    "       format(subsample_percentage, subsample_rowcounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>64034</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1251144218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>1028</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1225308749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>4639</td>\n",
       "      <td>4.0</td>\n",
       "      <td>998691698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>3100</td>\n",
       "      <td>2.0</td>\n",
       "      <td>994638190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>2278</td>\n",
       "      <td>2.0</td>\n",
       "      <td>994232903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0      11    64034     4.5  1251144218\n",
       "1      14     1028     3.0  1225308749\n",
       "2      17     4639     4.0   998691698\n",
       "3      22     3100     2.0   994638190\n",
       "4      24     2278     2.0   994232903"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join them back to original dataset to get ratings and time stamps, using userId and movieId as key\n",
    "subsampled_ratings = subsampled_ratings.join(ratings, ['userId','movieid'], 'inner')\n",
    "subsampled_ratings.persist()\n",
    "pd.DataFrame(subsampled_ratings.take(5), columns=subsampled_ratings.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into training, validation and test set <br>\n",
    "\n",
    "**Description**: <br><br>\n",
    "\n",
    "\n",
    "**Output**: 'train' - training set,  'val' - validation set, 'test' - test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: long (nullable = true)\n",
      " |-- movieId: long (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subsampled_ratings = subsampled_ratings.withColumn(\"timestamp\", ratings[\"timestamp\"].cast(T.TimestampType()))\n",
    "subsampled_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 10\n",
    "# test size\n",
    "test_size = 0.05\n",
    "def udf_user_limit(user_counts):\n",
    "    if user_counts < limit:\n",
    "        return -1\n",
    "    else:\n",
    "        if (user_counts * (1-test_size)) <= limit:\n",
    "            return limit\n",
    "        else:\n",
    "            return int(np.around(user_counts * (1-test_size)))\n",
    "user_limit = F.udf(udf_user_limit, T.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test function\n",
    "def train_test_split(data, col_to_split_on= 'userId', timestamp_col = 'timestamp'):\n",
    "    # original columns\n",
    "    orig_cols = data.columns\n",
    "    # define count col\n",
    "    count_col = 'count(' + col_to_split_on + ')'\n",
    "    # do user ratings count\n",
    "    counts_for_col = data.groupby(col_to_split_on).agg(F.count(col_to_split_on))\n",
    "    # generate limits on each user based on rules\n",
    "    limits = counts_for_col.withColumn('train_limit', user_limit(F.col(count_col)))\n",
    "    # remove users having less than limit no of ratings\n",
    "    limits_filtered = limits.filter(limits.train_limit > 0)\n",
    "    # generate row numbers based on temporality\n",
    "    data_row_num = data.withColumn(\"row_num\", F.row_number().over(Window.partitionBy(col_to_split_on).orderBy(timestamp_col)))\n",
    "    # join ratings and user counts dfs together\n",
    "    data_row_num = data_row_num.alias('a')\n",
    "    limits_filtered = limits_filtered.alias('b')\n",
    "    merged_data = data_row_num.join(limits_filtered,F.col('b.' + col_to_split_on) == F.col('a.' + col_to_split_on)).select([F.col('a.'+xx) for xx in data_row_num.columns] + [F.col('b.' + count_col),F.col('b.train_limit')])\n",
    "    # generate selection column based on number limit\n",
    "    final_train_test = merged_data.withColumn('selection', F.col('row_num') <= F.col('train_limit'))\n",
    "    # find train and test \n",
    "    train = final_train_test.filter(final_train_test.selection == True).select(orig_cols)\n",
    "    test = final_train_test.filter(final_train_test.selection == False).select(orig_cols)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check if test has more entities in any column\n",
    "def compatibility_test(train, test, cols_to_test = ['userId', 'movieId']):\n",
    "    cols_greater = []\n",
    "    for i in cols_to_test:\n",
    "        train_unique = train.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n",
    "        test_unique = test.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n",
    "        size = len(list(set(test_unique) - set(train_unique)))\n",
    "        print(\"Test has %d more %s\" %(size, i))\n",
    "        cols_greater.append(i)\n",
    "    return cols_greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18966365, 4)\n",
      "(998468, 4)\n",
      "Test has 391 more userId\n",
      "Test has 0 more movieId\n"
     ]
    }
   ],
   "source": [
    "traincv, test = train_test_split(ratings, col_to_split_on= 'movieId', timestamp_col='timestamp')\n",
    "print((traincv.count(), len(traincv.columns)))\n",
    "print((test.count(), len(test.columns)))\n",
    "cols_greater = compatibility_test(traincv, test, cols_to_test=['userId', 'movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18966365, 4)\n",
      "(980868, 4)\n",
      "Test has 0 more userId\n",
      "Test has 0 more movieId\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: double, timestamp: int]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in cols_greater:\n",
    "    traincv_unique = traincv.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n",
    "    test_unique = test.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n",
    "    unique_to_test = list(set(test_unique) - set(traincv_unique))\n",
    "    test = test[~test[i].isin(unique_to_test)]\n",
    "# After removing users/ movies not included in training set\n",
    "print((traincv.count(), len(traincv.columns)))\n",
    "print((test.count(), len(test.columns)))\n",
    "cols_greater = compatibility_test(traincv, test, cols_to_test=['userId', 'movieId'])\n",
    "test.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same logic on train / validation set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18018048, 4)\n"
     ]
    }
   ],
   "source": [
    "train, val = train_test_split(traincv, col_to_split_on= 'movieId', timestamp_col='timestamp')\n",
    "print((train.count(), len(train.columns)))\n",
    "print((val.count(), len(val.columns)))\n",
    "cols_greater = compatibility_test(train, val, cols_to_test=['userId', 'movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cols_greater:\n",
    "    train_unique = train.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n",
    "    val_unique = val.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n",
    "    unique_to_val = list(set(val_unique) - set(train_unique))\n",
    "    val = val[~val[i].isin(unique_to_val)]\n",
    "print((train.count(), len(train.columns)))\n",
    "print((val.count(), len(val.columns)))\n",
    "cols_greater = compatibility_test(train, val, cols_to_test=['userId', 'movieId'])\n",
    "train.persist()\n",
    "val.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric\n",
    "\n",
    "#### I. Regression Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metric(ratings, predicted_ratings):\n",
    "    \"\"\"\n",
    "        Calculate regression metrics\n",
    "        - root mean square error\n",
    "        - mean absoluate error\n",
    "        - r squared\n",
    "        - explained variance\n",
    "        by joining original and predicted rating dataset into \n",
    "        ((userId, movieId), (original_rating, predicted_rating)).\n",
    "        :ratings: original rating dataset. Format:   userId | movieId | rating\n",
    "        :predicted_ratings: predicted rating dataset\n",
    "    \"\"\"\n",
    "    ratings_tuple = ratings.rdd.map(lambda r: ((r.userId, r.movieId), r.rating))\n",
    "    predicted_ratings_tuple = predicted_ratings.rdd.map(lambda r: ((r.userId, r.movieId), r.rating))\n",
    "    score_and_labels = predicted_ratings_tuple.join(ratings_tuple).map(lambda tup: tup[1])\n",
    "    metrics = RegressionMetrics(score_and_labels)\n",
    "    print(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n",
    "    print(\"MAE = %s\" % metrics.meanAbsoluteError)\n",
    "    print(\"R-squared = %s\" % metrics.r2)\n",
    "    print(\"Explained Variance = %s\" % metrics.explainedVariance)\n",
    "    return metrics.rootMeanSquaredError, metrics.meanAbsoluteError, metrics.r2, metrics.explainedVariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Ranking Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking metrics\n",
    "# https://vinta.ws/code/spark-ml-cookbook-pyspark.html\n",
    "def ranking_metric(ratings, predicted_ratings, k=5):\n",
    "    \"\"\"\n",
    "        Calculate ranking metrics\n",
    "        - mean average precision\n",
    "        - precision at k\n",
    "        - normalized discounted continuous gain at k\n",
    "        by collecting recommended items for both datasets, which takes form of\n",
    "        ((userId, movieId), ([[predicted_item1, predicted_item2, ...], [item1, item2, ...]))\n",
    "        :ratings: original rating dataset. Format:   userId | movieId | rating\n",
    "        :predicted ratings: predicted rating dataset, same format\n",
    "        :k: top k positions used for comparison\n",
    "    \"\"\"\n",
    "    windowSpec = Window.partitionBy('userId').orderBy(col('rating').desc())\n",
    "    perUserPredictedItemsDF = predicted_ratings \\\n",
    "        .select('userId', 'movieId', 'rating', F.rank().over(windowSpec).alias('rank')) \\\n",
    "        .where('rank <= {0}'.format(k)) \\\n",
    "        .groupBy('userId') \\\n",
    "        .agg(expr('collect_list(movieId) as itemsId'))\n",
    "    \n",
    "    windowSpec = Window.partitionBy('userId').orderBy(col('rating').desc())\n",
    "    perUserActualItemsDF = ratings \\\n",
    "        .select('userId', 'movieId', 'rating', F.rank().over(windowSpec).alias('rank')) \\\n",
    "        .where('rank <= {0}'.format(k)) \\\n",
    "        .groupBy('userId') \\\n",
    "        .agg(expr('collect_list(movieId) as itemsId')) \n",
    "    \n",
    "    perUserItemsRDD = perUserPredictedItemsDF.join(perUserActualItemsDF, 'userId') \\\n",
    "        .rdd \\\n",
    "        .map(lambda row: (row[1], row[2]))\n",
    "    rankingMetrics = RankingMetrics(perUserItemsRDD)\n",
    "\n",
    "    print(\"mean Average Precision = %s\" %rankingMetrics.meanAveragePrecision)\n",
    "    print(\"Precision at k = %s\" %rankingMetrics.precisionAt(k))\n",
    "    print(\"NDCG at k = %s\" %rankingMetrics.ndcgAt(k))  \n",
    "    \n",
    "    return rankingMetrics.meanAveragePrecision, rankingMetrics.precisionAt(k), rankingMetrics.ndcgAt(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Area Under Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model <br>\n",
    "\n",
    "**Description:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel:\n",
    "\n",
    "    def __init__(self, user_column, item_column,ratings_column):\n",
    "        self.user_col = user_column\n",
    "        self.item_col = item_column\n",
    "        self.ratings_col = ratings_column\n",
    "\n",
    "    def __find_avg_of_col(self, data, column):\n",
    "        return data.select(F.mean(F.col(column))).collect()[0][0]\n",
    "\n",
    "    def __subtract_from_col(self, data, column, value):\n",
    "        return data.withColumn('normalized_' + column, F.col(column)-value)\n",
    "\n",
    "    def train(self, training_data):\n",
    "        user_col = self.user_col\n",
    "        item_col = self.item_col\n",
    "        ratings_col = self.ratings_col\n",
    "        user_bias = {}\n",
    "        item_bias = {}\n",
    "\n",
    "        # find average and calculate bias for user and item\n",
    "        avg_rating = self.__find_avg_of_col(training_data, ratings_col)\n",
    "        norm_training_data = self.__subtract_from_col(training_data, ratings_col, avg_rating)\n",
    "        user_bias = norm_training_data.groupby(user_col).agg(F.avg('normalized_' + ratings_col)).\\\n",
    "                                                    rdd.map(lambda x : (x[0],x[1])).collectAsMap()\n",
    "        item_bias = norm_training_data.groupby(item_col).agg(F.avg('normalized_' + ratings_col)).\\\n",
    "                                                    rdd.map(lambda x : (x[0],x[1])).collectAsMap()\n",
    "\n",
    "        self.training_data = norm_training_data\n",
    "        self.avg_rating = avg_rating\n",
    "        self.user_bias = user_bias\n",
    "        self.item_bias = item_bias\n",
    "    \n",
    "        return avg_rating, user_bias, item_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselineModel = BaselineModel(user_column='userId', item_column='movieId', ratings_column='rating')\n",
    "avg_rating, user_bias, item_bias = baselineModel.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udf_predict(user_id, item_id):\n",
    "    \"\"\"\n",
    "        Predict function: average rating + user bias + item bias. Handle\n",
    "        corner cases where there is missing user id and item id. \n",
    "    \"\"\"\n",
    "    if (user_id in user_bias) and (item_id in item_bias) :\n",
    "        return avg_rating + user_bias[user_id] + item_bias[item_id]\n",
    "    elif user_id in user_bias:\n",
    "        return avg_rating + user_bias[user_id]\n",
    "    elif item_id in item_bias:\n",
    "        return avg_rating + item_bias[item_id]\n",
    "    else: \n",
    "        return avg_rating\n",
    "def predict(test_data, user_column, item_column, ratings_column):\n",
    "    predict_udf = F.udf(udf_predict, T.FloatType())\n",
    "    return test_data.withColumn(ratings_column, predict_udf(F.col(user_column), F.col(item_column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Do cross join so we can predict on everything, not just the data that is available in the test set\n",
    "# user_vals = test.select('userId')\n",
    "# rating_vals = test.select('movieId')\n",
    "# predict_df = user_vals.crossJoin(rating_vals)\n",
    "predicted_ratings_baseline = predict(val, user_column='userId', item_column='movieId', ratings_column='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.913494648428\n",
      "MAE = 0.684286236834\n",
      "R-squared = 0.291004251236\n",
      "Explained Variance = 0.627566356625\n"
     ]
    }
   ],
   "source": [
    "rmse_baseline, mae_baseline, r2_baseline, explainedvar_baseline = \\\n",
    "regression_metric(val, predicted_ratings_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean Average Precision = 0.747433432528\n",
      "Precision at k = 0.503518355896\n",
      "NDCG at k = 0.846501517443\n"
     ]
    }
   ],
   "source": [
    "averageprecision_baseline, precision_baseline, ndcg_baseline = \\\n",
    "ranking_metric(val, predicted_ratings_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Based Model <br> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train.toPandas()\n",
    "val_df = test.toPandas()\n",
    "reader = Reader(rating_scale=(0.0,5.0))\n",
    "train_data = Dataset.load_from_df(train_df[['userId','movieId','rating']],reader)\n",
    "val_data = Dataset.load_from_df(val_df[['userId','movieId','rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name':'cosine', 'user_based':False}\n",
    "algo = KNNBasic(sim_options = sim_options)\n",
    "### Error \n",
    "algo.fit(train_data)\n",
    "predictions = algo.test(val_data)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Run: ALS with random chosen parameters\n",
    "als = ALS(maxIter=10, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "als_model = als.fit(train)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "predicted_ratings_als = als_model.transform(val)\n",
    "predicted_ratings_als = predicted_ratings_als.withColumn('rating', predicted_ratings_als.prediction).drop('prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.949870120165\n",
      "MAE = 0.717391207724\n",
      "R-squared = 0.233415437818\n",
      "Explained Variance = 0.785107962996\n"
     ]
    }
   ],
   "source": [
    "rmse_als, mae_als, r2_als, explainedvar_als= \\\n",
    "regression_metric(val, predicted_ratings_als)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean Average Precision = 0.75692714216\n",
      "Precision at k = 0.515680348528\n",
      "NDCG at k = 0.859847192553\n"
     ]
    }
   ],
   "source": [
    "averageprecision_als, precision_als, ndcg_als = \\\n",
    "ranking_metric(val, predicted_ratings_als)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CAUTION: THIS CELL TAKES LONG TO RUN\n",
    "Hyperparameter tuning using one validation set. \n",
    "\"\"\"\n",
    "\n",
    "grid = {'maxIter':[5], 'regParam': [1.0,2.0]}\n",
    "param_vals = []\n",
    "for key,val in grid.items():\n",
    "    param_vals.append(val)\n",
    "    \n",
    "final_results = dict()\n",
    "for i in itertools.product(*param_vals):\n",
    "    evaluation = [] \n",
    "    inputs = dict()\n",
    "    for j,(key,val) in enumerate(grid.items()):\n",
    "        inputs[key] = i[j]\n",
    "    inputs['userCol'] ='userId'\n",
    "    inputs['itemCol'] = 'movieId'\n",
    "    inputs['ratingCol'] = 'rating'\n",
    "    als = ALS(**inputs)\n",
    "    model = als.fit(train)\n",
    "    predictions = model.transform(val)\n",
    "    cv_rmse, cv_mae, cv_rw, cv_exp_var = \\\n",
    "    regression_metric(val, predictions)\n",
    "    evaluation.append({'rmse': cv_rmse})\n",
    "    final_results[i] = evaluation\n",
    "\n",
    "\n",
    "# If you want to run hyperparameter tuning using cross validation. This takes even longer time.\n",
    "# paramMap = ParamGridBuilder() \\\n",
    "#                     .addGrid(als.rank, [10, 50, 100]) \\\n",
    "#                     .addGrid(als.maxIter, [10]) \\\n",
    "#                     .addGrid(als.regParam, [0.01,0.001]) \\\n",
    "#                     .build()\n",
    "\n",
    "# evaluatorR = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\")\n",
    "\n",
    "# # Run cross-validation, and choose the best set of parameters.\n",
    "# cv = CrossValidator(estimator=als,\n",
    "#                             estimatorParamMaps=paramMap,\n",
    "#                             evaluator=evaluatorR,\n",
    "#                            numFolds=5)\n",
    "\n",
    "# cv_res = cv.fit(subsampled_train)\n",
    "# final_results\n",
    "# predicted_ratings_als = cv_res.bestModel.transform(subsampled_test)\n",
    "# predicted_ratings_als = predicted_ratings_als.withColumn('rating', predicted_ratings_als.prediction).drop('prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_als, mae_als, r2_als, explainedvar_als= \\\n",
    "regression_metric(val, predicted_ratings_als)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageprecision_als, precision_als, ndcg_als = \\\n",
    "ranking_metric(val, predicted_ratings_als)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric\n",
    "\n",
    "#### I. Ranking Metric\n",
    "\n",
    "**(i) AUC **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def areaUnderCurve(positiveData, bAllArtistIDs, predictFunction): \n",
    "#     // What this actually computes is AUC, per user. The result is actually something\n",
    "#     // that might be called \"mean AUC\".\n",
    "\n",
    "#     // Take held-out data as the \"positive\".\n",
    "#     // Make predictions for each of them, including a numeric score\n",
    "#     val positivePredictions = predictFunction(positiveData.select(\"user\", \"artist\")).\n",
    "#       withColumnRenamed(\"prediction\", \"positivePrediction\")\n",
    "\n",
    "#     // BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\n",
    "#     // small AUC problems, and it would be inefficient, when a direct computation is available.\n",
    "\n",
    "#     // Create a set of \"negative\" products for each user. These are randomly chosen\n",
    "#     // from among all of the other artists, excluding those that are \"positive\" for the user.\n",
    "    val negativeData = positiveData.select(\"user\", \"artist\").as[(Int,Int)].j\n",
    "      groupByKey { case (user, _) => user }.\n",
    "      flatMapGroups { case (userID, userIDAndPosArtistIDs) =>\n",
    "        val random = new Random()\n",
    "        val posItemIDSet = userIDAndPosArtistIDs.map { case (_, artist) => artist }.toSet\n",
    "        val negative = new ArrayBuffer[Int]()\n",
    "        val allArtistIDs = bAllArtistIDs.value\n",
    "        var i = 0\n",
    "#         // Make at most one pass over all artists to avoid an infinite loop.\n",
    "#         // Also stop when number of negative equals positive set size\n",
    "        while (i < allArtistIDs.length && negative.size < posItemIDSet.size) {\n",
    "          val artistID = allArtistIDs(random.nextInt(allArtistIDs.length))\n",
    "#           // Only add new distinct IDs\n",
    "          if (!posItemIDSet.contains(artistID)) {\n",
    "            negative += artistID\n",
    "          }\n",
    "          i += 1\n",
    "        }\n",
    "#         // Return the set with user ID added back\n",
    "        negative.map(artistID => (userID, artistID))\n",
    "      }.toDF(\"user\", \"artist\")\n",
    "\n",
    "#     // Make predictions on the rest:\n",
    "    val negativePredictions = predictFunction(negativeData).\n",
    "      withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
    "\n",
    "#     // Join positive predictions to negative predictions by user, only.\n",
    "#     // This will result in a row for every possible pairing of positive and negative\n",
    "#     // predictions within each user.\n",
    "    val joinedPredictions = positivePredictions.join(negativePredictions, \"user\").\n",
    "      select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\n",
    "\n",
    "#     // Count the number of pairs per user\n",
    "    val allCounts = joinedPredictions.\n",
    "      groupBy(\"user\").agg(count(lit(\"1\")).as(\"total\")).\n",
    "      select(\"user\", \"total\")\n",
    "#     // Count the number of correctly ordered pairs per user\n",
    "    val correctCounts = joinedPredictions.\n",
    "      filter($\"positivePrediction\" > $\"negativePrediction\").\n",
    "      groupBy(\"user\").agg(count(\"user\").as(\"correct\")).\n",
    "      select(\"user\", \"correct\")\n",
    "\n",
    "#     // Combine these, compute their ratio, and average over all users\n",
    "    val meanAUC = allCounts.join(correctCounts, Seq(\"user\"), \"left_outer\").\n",
    "      select($\"user\", (coalesce($\"correct\", lit(0)) / $\"total\").as(\"auc\")).\n",
    "      agg(mean(\"auc\")).\n",
    "      as[Double].first()\n",
    "\n",
    "    joinedPredictions.unpersist()\n",
    "\n",
    "    return meanAUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def areaUnderCurve(\n",
    "      positiveData: DataFrame,\n",
    "      bAllArtistIDs: Broadcast[Array[Int]],\n",
    "      predictFunction: (DataFrame => DataFrame)): Double = {\n",
    "\n",
    "#     // What this actually computes is AUC, per user. The result is actually something\n",
    "#     // that might be called \"mean AUC\".\n",
    "\n",
    "#     // Take held-out data as the \"positive\".\n",
    "#     // Make predictions for each of them, including a numeric score\n",
    "#     val positivePredictions = predictFunction(positiveData.select(\"user\", \"artist\")).\n",
    "#       withColumnRenamed(\"prediction\", \"positivePrediction\")\n",
    "\n",
    "#     // BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\n",
    "#     // small AUC problems, and it would be inefficient, when a direct computation is available.\n",
    "\n",
    "#     // Create a set of \"negative\" products for each user. These are randomly chosen\n",
    "#     // from among all of the other artists, excluding those that are \"positive\" for the user.\n",
    "    val negativeData = positiveData.select(\"user\", \"artist\").as[(Int,Int)].\n",
    "      groupByKey { case (user, _) => user }.\n",
    "      flatMapGroups { case (userID, userIDAndPosArtistIDs) =>\n",
    "        val random = new Random()\n",
    "        val posItemIDSet = userIDAndPosArtistIDs.map { case (_, artist) => artist }.toSet\n",
    "        val negative = new ArrayBuffer[Int]()\n",
    "        val allArtistIDs = bAllArtistIDs.value\n",
    "        var i = 0\n",
    "#         // Make at most one pass over all artists to avoid an infinite loop.\n",
    "#         // Also stop when number of negative equals positive set size\n",
    "        while (i < allArtistIDs.length && negative.size < posItemIDSet.size) {\n",
    "          val artistID = allArtistIDs(random.nextInt(allArtistIDs.length))\n",
    "#           // Only add new distinct IDs\n",
    "          if (!posItemIDSet.contains(artistID)) {\n",
    "            negative += artistID\n",
    "          }\n",
    "          i += 1\n",
    "        }\n",
    "#         // Return the set with user ID added back\n",
    "        negative.map(artistID => (userID, artistID))\n",
    "      }.toDF(\"user\", \"artist\")\n",
    "\n",
    "#     // Make predictions on the rest:\n",
    "    val negativePredictions = predictFunction(negativeData).\n",
    "      withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
    "\n",
    "#     // Join positive predictions to negative predictions by user, only.\n",
    "#     // This will result in a row for every possible pairing of positive and negative\n",
    "#     // predictions within each user.\n",
    "    val joinedPredictions = positivePredictions.join(negativePredictions, \"user\").\n",
    "      select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\n",
    "\n",
    "#     // Count the number of pairs per user\n",
    "    val allCounts = joinedPredictions.\n",
    "      groupBy(\"user\").agg(count(lit(\"1\")).as(\"total\")).\n",
    "      select(\"user\", \"total\")\n",
    "#     // Count the number of correctly ordered pairs per user\n",
    "    val correctCounts = joinedPredictions.\n",
    "      filter($\"positivePrediction\" > $\"negativePrediction\").\n",
    "      groupBy(\"user\").agg(count(\"user\").as(\"correct\")).\n",
    "      select(\"user\", \"correct\")\n",
    "    \n",
    "#     // Combine these, compute their ratio, and average over all users\n",
    "    val meanAUC = allCounts.join(correctCounts, Seq(\"user\"), \"left_outer\").\n",
    "      select($\"user\", (coalesce($\"correct\", lit(0)) / $\"total\").as(\"auc\")).\n",
    "      agg(mean(\"auc\")).\n",
    "      as[Double].first()\n",
    "\n",
    "    joinedPredictions.unpersist()\n",
    "\n",
    "    meanAUC\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "allUserId = subsampled_traincv.select('userId').distinct().rdd.map(lambda x: x[0]).collect()\n",
    "ballUserId = spark.sparkContext.broadcast(allUserId)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
