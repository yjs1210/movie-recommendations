{"nbformat_minor": 2, "cells": [{"source": "# Movie Recommender Project Report \n<br>\n<font size=4> Team members: </font> <br><br>\n<font size=3> James Jungsuk Lee [UNI] </font><br>\n<font size=3> Ujjwal Peshin [UNI]</font><br>\n<font size=3> Bowen Zhou [UNI]</font><br>\n<font size=3> Zhongling Jiang [UNI]</font>", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "import numpy as np\nimport pyspark\nfrom pyspark.sql import SparkSession\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nfrom pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import col, expr\nimport pyspark.sql.functions as F\nimport pyspark.sql.types as T\n\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n\nfrom surprise import AlgoBase\nfrom surprise import Dataset\nfrom surprise import accuracy\nfrom surprise import Reader\nfrom surprise import KNNBasic\nfrom surprise.model_selection import KFold\nfrom surprise.model_selection import train_test_split\nfrom surprise.model_selection import cross_validate\nimport pandas as pd\nimport random\n\nimport itertools\nRANDOM_SPLIT_SEED = 24\nTRAIN_PROPORTION = 0.9\nTEST_PROPORTION = 1 - TRAIN_PROPORTION", "outputs": [], "metadata": {}}, {"source": "### Answer to \nAs data scientists of a digital media company, state your objectives in building a\nrecommendation system. For example, what metrics do you care about, who is this system\nbuilt to serve (users or your boss?), and what business rules may you care to introduce?", "cell_type": "markdown", "metadata": {}}, {"source": "As a data scientist, you want to create a solution that serves the users but as well as can satisify your coworkers such as your boss. Having key stakeholders bought into your idea is integral in adding value to the company. So here we list out a few metrics and ideas that we can focus on and also point out who they serve.\n\nUsers: <br/>\nWe want our users to feel engaged with our content. That means we want to push out recommendations that they can relate to and enjoy. Within the context of our project, we can measure accuracy of our model such as RMSE. However, there can be some other things we can measure as well that's a bit out of scope of this project such as serendipity of our recommendations and implicit feedbacks such as how long they listen or watch our recommendations even if they don't explicitly rate them.\n    \nStakeholders: <br/>\nStakeholders want to make sure that the solutions that we recommend are indeed better than what they can do or have done in the past. They also want to understand how this system is affecting the product that they own. This means that measurements such as accuracy is also useful for stakeholders, but we can additionally focus on model methods that help them interpret the model's output. This means using simpler methods such as KNN or exploring Matrix Factorization methods to see if there can be any patterns in a reduced dimension space that can make sense to human rationality.", "cell_type": "markdown", "metadata": {}}, {"source": "## Part I: Data Preparation", "cell_type": "markdown", "metadata": {}}, {"source": "**Description: ** <br><br>", "cell_type": "markdown", "metadata": {}}, {"source": "### Create a smaller development set", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "spark = SparkSession.builder.appName('proj_1').getOrCreate()\nratings = spark.read.csv('gs://moviercommendation/ml-20m/ratings.csv', header = True, \n                         inferSchema=True).cache()\nratings.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp: integer (nullable = true)\n\n"}], "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "pd.DataFrame(ratings.take(5), columns=ratings.columns)", "outputs": [{"execution_count": 3, "output_type": "execute_result", "data": {"text/plain": "   userId  movieId  rating   timestamp\n0       1        2     3.5  1112486027\n1       1       29     3.5  1112484676\n2       1       32     3.5  1112484819\n3       1       47     3.5  1112484727\n4       1       50     3.5  1112484580", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n      <td>3.5</td>\n      <td>1112486027</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>29</td>\n      <td>3.5</td>\n      <td>1112484676</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>32</td>\n      <td>3.5</td>\n      <td>1112484819</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>47</td>\n      <td>3.5</td>\n      <td>1112484727</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>50</td>\n      <td>3.5</td>\n      <td>1112484580</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}], "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "random.seed(100)", "outputs": [], "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "def subsample(movies, n, p):\n    \"\"\"\n    The function subsample the list of movies that each user rates, based on following rule:\n    (i) If user has rated fewer than n movies, we keep all ratings\n    (ii) If user has rated a lot of movies, we keep only p percent of them via random selection\n    (iii) If number of movies * p percent is less than n, we randomly sample n movies and \n         keep these ratings\n    \"\"\"\n    if len(movies) <= n:\n        return movies\n    elif int(p * len(movies)) <= n:\n        return random.sample(movies, n)\n    else:\n        return random.sample(movies, int(p * len(movies)))\nN = 5\nP = 0.2    \n# collect all movies each user has rated\nratings_rdd = ratings.select(['userId', 'movieId']).rdd.map(list)\nusers_rated_movies = ratings_rdd.groupByKey().mapValues(list)\n\n# call subsample on rated movies\nsubsampled_users_rated_movies= users_rated_movies.mapValues(lambda m: subsample(m, N, P))\n\n# Now convert these ratings back to dataframe\nsubsampled_ratings = subsampled_users_rated_movies.flatMapValues(lambda x: x)\nsubsampled_ratings = spark.createDataFrame(subsampled_ratings, ['userId', 'movieId'])\n\n# check size of the subsampled dataset\nsubsample_rowcounts = subsampled_ratings.count()\nsubsample_percentage = subsample_rowcounts * 1.0 / ratings.count() * 100\nprint ('The subsampled dataset is {0}% size of the original dataset, has {1} rows'.\\\n       format(subsample_percentage, subsample_rowcounts))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "The subsampled dataset is 19.8135544518% size of the original dataset, has 3962763 rows\n"}], "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "# Join them back to original dataset to get ratings and time stamps, using userId and movieId as key\nsubsampled_ratings = subsampled_ratings.join(ratings, ['userId','movieid'], 'inner')\nsubsampled_ratings.persist()\npd.DataFrame(subsampled_ratings.take(5), columns=subsampled_ratings.columns)", "outputs": [{"execution_count": 6, "output_type": "execute_result", "data": {"text/plain": "   userId  movieId  rating   timestamp\n0      10     1222     3.0   943497235\n1      17     4639     4.0   998691698\n2      19       61     4.0   855176926\n3      24     3948     4.0   994821147\n4      25      590     4.0  1277963382", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>1222</td>\n      <td>3.0</td>\n      <td>943497235</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17</td>\n      <td>4639</td>\n      <td>4.0</td>\n      <td>998691698</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19</td>\n      <td>61</td>\n      <td>4.0</td>\n      <td>855176926</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>24</td>\n      <td>3948</td>\n      <td>4.0</td>\n      <td>994821147</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>25</td>\n      <td>590</td>\n      <td>4.0</td>\n      <td>1277963382</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}], "metadata": {}}, {"source": "### Divide into training, validation and test set <br>\n\n**Description**: <br><br>\n\n\n**Output**: 'train' - training set,  'val' - validation set, 'test' - test set", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "subsampled_ratings = subsampled_ratings.withColumn(\"timestamp\", ratings[\"timestamp\"].cast(T.TimestampType()))\nsubsampled_ratings.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- userId: long (nullable = true)\n |-- movieId: long (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n\n"}], "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "limit = 10\n# test size\ntest_size = 0.05\ndef udf_user_limit(user_counts):\n    if user_counts < limit:\n        return -1\n    else:\n        if (user_counts * (1-test_size)) <= limit:\n            return limit\n        else:\n            return int(np.around(user_counts * (1-test_size)))\nuser_limit = F.udf(udf_user_limit, T.IntegerType())", "outputs": [], "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "# train-test function\ndef train_test_split(data, col_to_split_on= 'userId', timestamp_col = 'timestamp'):\n    # original columns\n    orig_cols = data.columns\n    # define count col\n    count_col = 'count(' + col_to_split_on + ')'\n    # do user ratings count\n    counts_for_col = data.groupby(col_to_split_on).agg(F.count(col_to_split_on))\n    # generate limits on each user based on rules\n    limits = counts_for_col.withColumn('train_limit', user_limit(F.col(count_col)))\n    # remove users having less than limit no of ratings\n    limits_filtered = limits.filter(limits.train_limit > 0)\n    # generate row numbers based on temporality\n    data_row_num = data.withColumn(\"row_num\", F.row_number().over(Window.partitionBy(col_to_split_on).orderBy(timestamp_col)))\n    # join ratings and user counts dfs together\n    data_row_num = data_row_num.alias('a')\n    limits_filtered = limits_filtered.alias('b')\n    merged_data = data_row_num.join(limits_filtered,F.col('b.' + col_to_split_on) == F.col('a.' + col_to_split_on)).select([F.col('a.'+xx) for xx in data_row_num.columns] + [F.col('b.' + count_col),F.col('b.train_limit')])\n    # generate selection column based on number limit\n    final_train_test = merged_data.withColumn('selection', F.col('row_num') <= F.col('train_limit'))\n    # find train and test \n    train = final_train_test.filter(final_train_test.selection == True).select(orig_cols)\n    test = final_train_test.filter(final_train_test.selection == False).select(orig_cols)\n    return train, test", "outputs": [], "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "# to check if test has more entities in any column\ndef compatibility_test(train, test, cols_to_test = ['userId', 'movieId']):\n    cols_greater = []\n    for i in cols_to_test:\n        train_unique = train.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n        test_unique = test.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n        size = len(list(set(test_unique) - set(train_unique)))\n        print(\"Test has %d more %s\" %(size, i))\n        cols_greater.append(i)\n    return cols_greater", "outputs": [], "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "traincv, test = train_test_split(ratings, col_to_split_on= 'movieId', timestamp_col='timestamp')\nprint((traincv.count(), len(traincv.columns)))\nprint((test.count(), len(test.columns)))\ncols_greater = compatibility_test(traincv, test, cols_to_test=['userId', 'movieId'])", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(18966365, 4)\n(998468, 4)\nTest has 391 more userId\nTest has 0 more movieId\n"}], "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "for i in cols_greater:\n    traincv_unique = traincv.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n    test_unique = test.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n    unique_to_test = list(set(test_unique) - set(traincv_unique))\n    test = test[~test[i].isin(unique_to_test)]\n# After removing users/ movies not included in training set\nprint((traincv.count(), len(traincv.columns)))\nprint((test.count(), len(test.columns)))\ncols_greater = compatibility_test(traincv, test, cols_to_test=['userId', 'movieId'])\ntest.persist()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(18966365, 4)\n(980868, 4)\nTest has 0 more userId\nTest has 0 more movieId\n"}, {"execution_count": 12, "output_type": "execute_result", "data": {"text/plain": "DataFrame[userId: int, movieId: int, rating: double, timestamp: int]"}, "metadata": {}}], "metadata": {}}, {"source": "Repeat the same logic on train / validation set split", "cell_type": "markdown", "metadata": {}}, {"execution_count": 46, "cell_type": "code", "source": "train, val = train_test_split(traincv, col_to_split_on= 'movieId', timestamp_col='timestamp')\nprint((train.count(), len(train.columns)))\nprint((val.count(), len(val.columns)))\ncols_greater = compatibility_test(train, val, cols_to_test=['userId', 'movieId'])", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(18018048, 4)\n(948317, 4)\nTest has 584 more userId\nTest has 0 more movieId\n"}], "metadata": {}}, {"execution_count": 47, "cell_type": "code", "source": "for i in cols_greater:\n    train_unique = train.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n    val_unique = val.select([i]).distinct().rdd.map(lambda x: x[0]).collect()\n    unique_to_val = list(set(val_unique) - set(train_unique))\n    val = val[~val[i].isin(unique_to_val)]\nprint((train.count(), len(train.columns)))\nprint((val.count(), len(val.columns)))\ncols_greater = compatibility_test(train, val, cols_to_test=['userId', 'movieId'])\ntrain.persist()\nval.persist()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(18018048, 4)\n(945174, 4)\nTest has 0 more userId\nTest has 0 more movieId\n"}, {"execution_count": 47, "output_type": "execute_result", "data": {"text/plain": "DataFrame[userId: int, movieId: int, rating: double, timestamp: int]"}, "metadata": {}}], "metadata": {}}, {"source": "### Evaluation Metrics\n\nWe explored mainly two type of evaluation metrics on our recommender system -- regression metrics and ranking metrics.\nThe problem setting can be described as the following: \n\nAssume U is a set of M users, $U=\\{u_0,u_1,...,u_{M\u22121}\\}$. \n\nEach user $u_i$ having a set of N ground truth relevant movies $Di=\\{d_0,d_1,...,d_{N\u22121}\\}$ and a list of Q recommended movies, in order of decreasing relevance $Ri=\\{r_0,r_1,...,r_{Q\u22121}\\}$\n\nOur goal is to evaluate how relevant our recommended movies are for each user. The relevance of the sets and the effectiveness of the algorithms can be measured using the metrics listed below.\n\nBefore diving into definition of metrics, we define a indicator function which, provided a recommended document and a set of ground truth relevant documents, returns a 0/1 score to indicate relevance.\n\n$rel_D(r)$ = \\begin{cases}\n    1,& \\text{if } r\\in D\\\\\n    0,              & \\text{otherwise}\n\\end{cases}\n\n\n#### I. Regression Metric\n\n* Root Mean Square Error (RMSE): $\\frac{\\sqrt{\\sum_{i=0}^{N - 1}e_{i}^2}}{N}$\n* Mean Absolute Error (MAE): $\\frac{\\sum_{i=0}^{N - 1}|e_{i}|}{N}$\n* R-squared: $1 - \\frac{\\sum_{i=0}^{N - 1} (e_i)^2 }{\\sum_{i=0}^{N - 1} (y_i - \\bar{y}_i)^2}$\n* Explained Variance: $1 - \\frac{Var(y - \\hat{y})}{Var(y)} $", "cell_type": "markdown", "metadata": {}}, {"execution_count": 54, "cell_type": "code", "source": "def regression_metric(ratings, predicted_ratings):\n    \"\"\"\n        Calculate regression metrics\n        - root mean square error\n        - mean absoluate error\n        - r squared\n        - explained variance\n        by joining original and predicted rating dataset into \n        ((userId, movieId), (original_rating, predicted_rating)).\n        :ratings: original rating dataset. Format:   userId | movieId | rating\n        :predicted_ratings: predicted rating dataset\n    \"\"\"\n    ratings_tuple = ratings.rdd.map(lambda r: ((r.userId, r.movieId), r.rating))\n    predicted_ratings_tuple = predicted_ratings.rdd.map(lambda r: ((r.userId, r.movieId), r.rating))\n    score_and_labels = predicted_ratings_tuple.join(ratings_tuple).map(lambda tup: tup[1])\n    metrics = RegressionMetrics(score_and_labels)\n    print(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n    print(\"MAE = %s\" % metrics.meanAbsoluteError)\n    print(\"R-squared = %s\" % metrics.r2)\n    print(\"Explained Variance = %s\" % metrics.explainedVariance)\n    return metrics.rootMeanSquaredError, metrics.meanAbsoluteError, metrics.r2, metrics.explainedVariance", "outputs": [], "metadata": {}}, {"source": "#### II. Ranking Metric\n\n* Precision @ k: measures on average what proportion of first k recommended movies to users are contained in true set of relevant movies to each user, i.e.<br>\n\n<font size=2.5>$p(k) = \\frac{1}{M}\\sum_{i=0}^{M-1}\\frac{1}{k}\\sum_{j=0}^{min(|D|, k)-1}rel_{D_i}(R_i(j))$</font>\n\n* Mean Average Precision @ k: measures similar quantity as Precision @ k except imposing penalty regarding order of recommendation, i.e. <br>\n\n<font size=2.5>$p(k) = \\frac{1}{M}\\sum_{i=0}^{M-1}\\frac{1}{k}\\sum_{j=0}^{min(|D|, k)-1}\\frac{rel_{D_i}(R_i(j))}{j + 1}$</font>\n\n* Normalized Discounted Continuous Gain (NDCG): measures how many of the first k recommended moveis are in the set of true relevant documents averaged across all users, while taking into account the order of the recommendations (movies are assumed to be in order of decreasing relevance).\n\n<font size=2.5>$NDCG(k) = \\frac{1}{M}\\sum_{i=0}^{M-1} \\frac{1}{IDCG(D_i, k)}\\sum_{j=0}^{n-1}\\frac{rel_{D_i}(R_i(j))}{ln(j + 1)} $<font> <br>\nwhere $IDCG(k) = \\sum_{j=0}^{min(|D|, k)-1}\\frac{rel_{D_i}(R_i(j))}{ln(j + 1)}$ represents best possible attained by the most ideal ranking.\n    \n**Procedure**:\n\n(i) Filter out movies with rating score < r, because we are only interested in evaluating relevant movies, or 'good' recommendations. \n\n(ii) Sort ground truths and predictions by ratings (predicted ratings) and retrieve top k movies as reccommended items for a user. If in test set, a user rates fewer than k movies, then only retrieve all his rated movies.\n\n\n(iii) Calculate specific ranking metric on test set and predicted set.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 55, "cell_type": "code", "source": "# Ranking metrics\n# https://vinta.ws/code/spark-ml-cookbook-pyspark.html\ndef ranking_metric(ratings, predicted_ratings, r=3, k=5):\n    \"\"\"\n        Calculate ranking metrics\n        - mean average precision\n        - precision at k\n        - normalized discounted continuous gain at k\n        by collecting recommended items and then comparing them to relevant groud truth items, \n        which takes form of\n        ((userId, movieId), ([[predicted_item1, predicted_item2, ...], [item1, item2, ...]))\n        :ratings: original rating dataset. Format:   userId | movieId | rating\n        :predicted ratings: predicted rating dataset, same format\n        :r: threshold for rating that defines a 'relevant' movie\n        :k: top k positions used for comparison\n    \"\"\"\n    windowSpec = Window.partitionBy('userId').orderBy(col('rating').desc())\n    perUserPredictedItemsDF = predicted_ratings \\\n        .where('rating >= {0}'.format(r)) \\\n        .select('userId', 'movieId', 'rating', F.rank().over(windowSpec).alias('rank')) \\\n        .where('rank <= {0}'.format(k)) \\\n        .groupBy('userId') \\\n        .agg(expr('collect_list(movieId) as itemsId'))\n    \n    windowSpec = Window.partitionBy('userId').orderBy(col('rating').desc())\n    perUserActualItemsDF = ratings \\\n        .where('rating >= {0}'.format(r)) \\\n        .select('userId', 'movieId', 'rating', F.rank().over(windowSpec).alias('rank')) \\\n        .where('rank <= {0}'.format(k)) \\\n        .groupBy('userId') \\\n        .agg(expr('collect_list(movieId) as itemsId')) \n    \n    perUserItemsRDD = perUserPredictedItemsDF.join(perUserActualItemsDF, 'userId') \\\n        .rdd \\\n        .map(lambda row: (row[1], row[2]))\n    rankingMetrics = RankingMetrics(perUserItemsRDD)\n\n    print(\"mean Average Precision = %s\" %rankingMetrics.meanAveragePrecision)\n    print(\"Precision at k = %s\" %rankingMetrics.precisionAt(k))\n    print(\"NDCG at k = %s\" %rankingMetrics.ndcgAt(k))  \n    \n    return rankingMetrics.meanAveragePrecision, rankingMetrics.precisionAt(k), rankingMetrics.ndcgAt(k)", "outputs": [], "metadata": {}}, {"source": "#### III. Coverage", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "def user_coverage_metric(ratings, predicted_ratings, r=3, k=5):\n    windowSpec = Window.partitionBy('userId').orderBy(col('rating').desc())\n    perUserPredictedItemsDF = predicted_ratings_baseline \\\n        .where('rating >= {0}'.format(r)) \\\n        .select(col('userId').alias('p_userId'), col('movieId').alias('p_movieId'), \\\n                F.col('rating').alias('p_ratings'), F.rank().over(windowSpec).alias('rank'))  \n    # join with test set ratings and rename columns\n    joined = perUserPredictedItemsDF \\\n        .join(val, (perUserPredictedItemsDF.p_userId ==val.userId) & (perUserPredictedItemsDF.p_movieId == val.movieId))     \n    # the number of users have above threshold, 'successful' recommendations\n    covered_users = joined.where('rating >= {0}'.format(r)) \\\n        .groupBy('p_userId') \\\n        .agg(F.countDistinct('p_movieId').alias('count')) \\\n        .where('count >= {0}'.format(k)) \\\n        .agg(F.countDistinct('p_userId'))    \n    covered_users = covered_users.collect()[0][0]\n    # the numeber of user have above threshold groudtruths \n    actual = val.where('rating >= {0}'.format(r)) \\\n        .groupBy('userId') \\\n        .agg(F.countDistinct('movieId').alias('count')) \\\n        .where('count >= {0}'.format(k)) \\\n        .agg(F.countDistinct('userId'))\n    total_users = actual.collect()[0][0]\n    \n    user_coverage = covered_users * 1.0 / total_users\n    return user_coverage\n            \ndef item_coverage_metric(ratings, predicted_ratings, r=3, k=5):\n    pass\ndef catelog_coverage_metric(ratings, predicted_ratings, r=3, k=5):\n    windowSpec = Window.partitionBy('userId').orderBy(col('rating').desc())\n    perUserPredictedItemsDF = predicted_ratings \\\n        .where('rating >= {0}'.format(r)) \\\n        .select('userId', 'movieId', 'rating', F.rank().over(windowSpec).alias('rank')) \\\n        .where('rank <= {0}'.format(k)) \\\n        .agg(F.countDistinct('movieId'))\n    num_rec_movies = perUserPredictedItemsDF.collect()[0][0]\n    \n    perUserActualItemsDF = ratings \\\n        .select('movieId') \\\n        .agg(F.countDistinct('movieId'))\n    num_total_movies = perUserActualItemsDF.collect()[0][0]\n    return num_rec_movies * 1.0 / num_total_movies", "outputs": [], "metadata": {}}, {"source": "### Baseline Model <br>\n\n**Description:**", "cell_type": "markdown", "metadata": {}}, {"execution_count": 79, "cell_type": "code", "source": "class BaselineModel:\n\n    def __init__(self, user_column, item_column,ratings_column):\n        self.user_col = user_column\n        self.item_col = item_column\n        self.ratings_col = ratings_column\n\n    def __find_avg_of_col(self, data, column):\n        return data.select(F.mean(F.col(column))).collect()[0][0]\n\n    def __subtract_from_col(self, data, column, value):\n        return data.withColumn('normalized_' + column, F.col(column)-value)\n\n    def train(self, training_data):\n        user_col = self.user_col\n        item_col = self.item_col\n        ratings_col = self.ratings_col\n        user_bias = {}\n        item_bias = {}\n\n        # find average and calculate bias for user and item\n        avg_rating = self.__find_avg_of_col(training_data, ratings_col)\n        norm_training_data = self.__subtract_from_col(training_data, ratings_col, avg_rating)\n        user_bias = norm_training_data.groupby(user_col).agg(F.avg('normalized_' + ratings_col)).\\\n                                                    rdd.map(lambda x : (x[0],x[1])).collectAsMap()\n        item_bias = norm_training_data.groupby(item_col).agg(F.avg('normalized_' + ratings_col)).\\\n                                                    rdd.map(lambda x : (x[0],x[1])).collectAsMap()\n\n        self.training_data = norm_training_data\n        self.avg_rating = avg_rating\n        self.user_bias = user_bias\n        self.item_bias = item_bias\n    \n        return avg_rating, user_bias, item_bias", "outputs": [], "metadata": {}}, {"execution_count": 80, "cell_type": "code", "source": "baselineModel = BaselineModel(user_column='userId', item_column='movieId', ratings_column='rating')\navg_rating, user_bias, item_bias = baselineModel.train(train)", "outputs": [], "metadata": {}}, {"execution_count": 81, "cell_type": "code", "source": "def udf_predict(user_id, item_id):\n    \"\"\"\n        Predict function: average rating + user bias + item bias. Handle\n        corner cases where there is missing user id and item id. \n    \"\"\"\n    if (user_id in user_bias) and (item_id in item_bias) :\n        return avg_rating + user_bias[user_id] + item_bias[item_id]\n    elif user_id in user_bias:\n        return avg_rating + user_bias[user_id]\n    elif item_id in item_bias:\n        return avg_rating + item_bias[item_id]\n    else: \n        return avg_rating\ndef predict(test_data, user_column, item_column, ratings_column):\n    predict_udf = F.udf(udf_predict, T.FloatType())\n    return test_data.withColumn(ratings_column, predict_udf(F.col(user_column), F.col(item_column)))", "outputs": [], "metadata": {}}, {"execution_count": 82, "cell_type": "code", "source": "###Do cross join so we can predict on everything, not just the data that is available in the test set\n# user_vals = test.select('userId')\n# rating_vals = test.select('movieId')\n# predict_df = user_vals.crossJoin(rating_vals)\npredicted_ratings_baseline = predict(val, user_column='userId', item_column='movieId', ratings_column='rating')", "outputs": [], "metadata": {}}, {"execution_count": 83, "cell_type": "code", "source": "rmse_baseline, mae_baseline, r2_baseline, explainedvar_baseline = \\\nregression_metric(val, predicted_ratings_baseline)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "RMSE = 0.901833333661\nMAE = 0.670538868999\nR-squared = 0.245409209469\nExplained Variance = 0.586994875903\n"}], "metadata": {}}, {"execution_count": 86, "cell_type": "code", "source": "averageprecision_baseline, precision_baseline, ndcg_baseline = \\\nranking_metric(val, predicted_ratings_baseline)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "mean Average Precision = 0.55839090148\nPrecision at k = 0.50094468816\nNDCG at k = 0.716480068419\n"}], "metadata": {"scrolled": true}}, {"source": "### Item Based Model <br> <br>", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "train_df = train.toPandas()\nval_df = test.toPandas()\nreader = Reader(rating_scale=(0.0,5.0))\ntrain_data = Dataset.load_from_df(train_df[['userId','movieId','rating']],reader)\nval_data = Dataset.load_from_df(val_df[['userId','movieId','rating']], reader)", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "sim_options = {'name':'cosine', 'user_based':False}\nalgo = KNNBasic(sim_options = sim_options)\n### Error \nalgo.fit(train_data)\npredictions = algo.test(val_data)\naccuracy.rmse(predictions)", "outputs": [], "metadata": {}}, {"source": "### Matrix Factorization Model", "cell_type": "markdown", "metadata": {}}, {"execution_count": 50, "cell_type": "code", "source": "# Example Run: ALS with random chosen parameters\nals = ALS(maxIter=10, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n          coldStartStrategy=\"drop\")\nals_model = als.fit(train)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n                                predictionCol=\"prediction\")\npredicted_ratings_als = als_model.transform(val)\npredicted_ratings_als = \\\n    predicted_ratings_als.withColumn('rating', predicted_ratings_als.prediction).drop('prediction')", "outputs": [], "metadata": {}}, {"execution_count": 51, "cell_type": "code", "source": "rmse_als, mae_als, r2_als, explainedvar_als= \\\nregression_metric(val, predicted_ratings_als)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "RMSE = 0.895115118791\nMAE = 0.670517276087\nR-squared = 0.256609993978\nExplained Variance = 0.691040291716\n"}], "metadata": {}}, {"execution_count": 53, "cell_type": "code", "source": "# Re-run the cell when there is an error\naverageprecision_als, precision_als, ndcg_als = \\\nranking_metric(val, predicted_ratings_als)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "mean Average Precision = 0.529251556357\nPrecision at k = 0.496996445356\nNDCG at k = 0.702288725705\n"}], "metadata": {}}, {"source": "#### Hyperparameter Tuning", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nCAUTION: THIS CELL TAKES LONG TO RUN\nHyperparameter tuning using one validation set. \n\"\"\"\n\ngrid = {'maxIter':[5, 20], 'rank':[20, 100], 'regParam': [0.1,2.0]}\nparam_vals = []\nfor key,v in grid.items():\n    param_vals.append(v)\n    \nfinal_results = dict()\nfor i in itertools.product(*param_vals):\n    evaluation = []\n    \n    inputs = dict()\n    for j,(key,v) in enumerate(grid.items()):\n        inputs[key] = i[j]\n    inputs['userCol'] ='userId'\n    inputs['itemCol'] = 'movieId'\n    inputs['ratingCol'] = 'rating'\n    inputs['coldStartStrategy'] = 'drop'\n    als = ALS(**inputs)\n    model = als.fit(train)\n    predictions = model.transform(val)\n    predictions = predictions.withColumn('rating', predictions.prediction).drop('prediction')\n    cv_rmse, cv_mae, cv_rw, cv_exp_var = \\\n    regression_metric(val, predictions)\n    evaluation.append({'rmse': cv_rmse})\n    final_results[i] = evaluation\n\n# If you want to run hyperparameter tuning using cross validation. This takes even longer time.\n# paramMap = ParamGridBuilder() \\\n#                     .addGrid(als.rank, [10, 50, 100]) \\\n#                     .addGrid(als.maxIter, [10]) \\\n#                     .addGrid(als.regParam, [0.01,0.001]) \\\n#                     .build()\n\n# evaluatorR = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\")\n\n# # Run cross-validation, and choose the best set of parameters.\n# cv = CrossValidator(estimator=als,\n#                             estimatorParamMaps=paramMap,\n#                             evaluator=evaluatorR,\n#                            numFolds=5)\n\n# cv_res = cv.fit(subsampled_train)\n# final_results\n# predicted_ratings_als = cv_res.bestModel.transform(subsampled_test)\n# predicted_ratings_als = predicted_ratings_als.withColumn('rating', predicted_ratings_als.prediction).drop('prediction')", "outputs": [{"output_type": "stream", "name": "stdout", "text": "RMSE = 0.826376335075\nMAE = 0.63888632522\nR-squared = 0.366400717598\nExplained Variance = 0.453199939853\nRMSE = 0.814801693954\nMAE = 0.625143453446\nR-squared = 0.384025434982\nExplained Variance = 0.449409340468\nRMSE = 0.847490364453\nMAE = 0.665800444686\nR-squared = 0.333609999767\nExplained Variance = 0.468496805383\nRMSE = 0.815419794248\nMAE = 0.626206366059\nR-squared = 0.383090536409\nExplained Variance = 0.449251504992\nRMSE = 2.74638708455\nMAE = 2.5899897181\nR-squared = -5.99813682225\nExplained Variance = 6.71617530607\nRMSE = 2.08478757742\nMAE = 1.92697289161\nR-squared = -3.03257593623\nExplained Variance = 3.64372333117\nRMSE = 2.75034227353\nMAE = 2.59381046904\nR-squared = -6.01830796612\nExplained Variance = 6.73441140285\nRMSE = 2.08478757683\nMAE = 1.92697289108\nR-squared = -3.03257593395\nExplained Variance = 3.64372332981\n"}], "metadata": {}}, {"execution_count": 71, "cell_type": "code", "source": "final_results", "outputs": [{"execution_count": 71, "output_type": "execute_result", "data": {"text/plain": "{(0.1, 20, 5): [{'rmse': 0.8263763350747986}],\n (0.1, 20, 20): [{'rmse': 0.8148016939541032}],\n (0.1, 100, 5): [{'rmse': 0.8474903644526278}],\n (0.1, 100, 20): [{'rmse': 0.8154197942484954}],\n (2.0, 20, 5): [{'rmse': 2.7463870845485077}],\n (2.0, 20, 20): [{'rmse': 2.084787577417859}],\n (2.0, 100, 5): [{'rmse': 2.7503422735344567}],\n (2.0, 100, 20): [{'rmse': 2.0847875768277393}]}"}, "metadata": {}}], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# averageprecision_als, precision_als, ndcg_als = \\\n# ranking_metric(val, predicted_ratings_als)", "outputs": [], "metadata": {}}, {"source": "## Performance Comparison on Test Set", "cell_type": "markdown", "metadata": {}}, {"execution_count": 85, "cell_type": "code", "source": "# combine train and cv\n\n\n# run three models on traincv and evaluate on test\n\n\n# Create visualization of metrics change", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pyspark", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.14", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}